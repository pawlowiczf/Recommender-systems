{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium 4 - rekomendacje dla portali informacyjnych\n",
    "\n",
    "## Przygotowanie\n",
    "\n",
    " * pobierz i wypakuj dataset: https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip\n",
    "   * więcej możesz poczytać tutaj: https://learn.microsoft.com/en-us/azure/open-datasets/dataset-microsoft-news\n",
    " * [opcjonalnie] Utwórz wirtualne środowisko\n",
    " `python3 -m venv ./recsyslab4`\n",
    " * zainstaluj potrzebne biblioteki:\n",
    " `pip install nltk sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 1. - przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Filip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importujemy wszystkie potrzebne pakiety\n",
    "\n",
    "import codecs\n",
    "from collections import defaultdict # mozesz uzyc zamiast zwyklego slownika, rozwaz wplyw na czas obliczen\n",
    "import math\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# mozesz uzyc do obliczania najbardziej podobnych tekstow zamiast liczenia \"na piechote\"\n",
    "# ale pamietaj o dostosowaniu formatu danych\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# na potrzeby wizualizacji\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definiujemy potrzebne zmienne\n",
    "\n",
    "PATH = './'\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51282\n"
     ]
    }
   ],
   "source": [
    "# wczytujemy metadane artykulow\n",
    "\n",
    "def parse_news_entry(entry):\n",
    "    news_id, category, subcategory, title, abstract = entry.split('\\t')[:5]\n",
    "    return {\n",
    "        'news_id': news_id,\n",
    "        'category': category,\n",
    "        'subcategory': subcategory,\n",
    "        'title': title,\n",
    "        'abstract': abstract\n",
    "    }\n",
    "\n",
    "def get_news_metadata():\n",
    "    with codecs.open(f'{PATH}/news.tsv', 'r', 'UTF-8') as f:\n",
    "        raw = [x for x in f.read().split('\\n') if x]\n",
    "        parsed_entries = [parse_news_entry(entry) for entry in raw]\n",
    "        return {x['news_id']: x for x in parsed_entries}\n",
    "\n",
    "news = get_news_metadata()\n",
    "news_ids = sorted(list(news.keys()))\n",
    "news_indices = {x[1]: x[0] for x in enumerate(news_ids)}\n",
    "print(len(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "def parse_history_entry(entry):\n",
    "    _id, user_id, _time, history, _impressions = entry.split('\\t')\n",
    "    history = [x for x in history.split() if x]\n",
    "    return user_id, history\n",
    "\n",
    "def get_users_history():\n",
    "    with codecs.open(f'{PATH}/behaviors.tsv', 'r', 'UTF-8') as f:\n",
    "        lines = [x for x in f.read().split('\\n') if x]\n",
    "        entries = [parse_history_entry(x) for x in lines]\n",
    "        return dict(entries)\n",
    "\n",
    "users_history = get_users_history()\n",
    "test_users = 'U53231', 'U89744', 'U10045', 'U92486', 'U70879'\n",
    "print(len(users_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 2. - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizujemy teksty na potrzeby dalszego przetwarzania\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # zamieniamy wszystkie ciagi bialych znakow na pojedyncze spacje\n",
    "    # usuwamy znaki interpunkcyjne\n",
    "    # usuwamy wszystkie liczby\n",
    "    # podmieniamy wszystkie wielkie litery\n",
    "    # dzielimy na tokeny\n",
    "    # usuwamy stopwords\n",
    "\n",
    "    MapChain = [\n",
    "        lambda text: re.sub(r'\\s+', ' ', text),\n",
    "        lambda text: re.sub(r'[^\\w\\s]', '', text),\n",
    "        lambda text: re.sub(r'\\d+', '', text),\n",
    "        lambda text: text.lower(),\n",
    "        lambda text: [token for token in text.split() if token not in STOPWORDS]\n",
    "    ]\n",
    "    \n",
    "    return reduce(lambda acc, func: func(acc), MapChain, text)\n",
    "#\n",
    "\n",
    "def stem_texts(corpus):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [[stemmer.stem(word) for word in preprocess_text(text)] for text in corpus]\n",
    "\n",
    "texts = [news[news_id]['abstract'] for news_id in news_ids]\n",
    "stemmed_texts = stem_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I think we have a really good team, and a team that can really do some special, good things because that group is very close in there.\" - Brian Schmetzer\n",
      "\n",
      "think realli good team team realli special good thing group close brian schmetzer\n"
     ]
    }
   ],
   "source": [
    "# porownajmy teksty przed i po przetworzeniu\n",
    "\n",
    "print(texts[2] + '\\n')\n",
    "print(' '.join(stemmed_texts[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41727\n"
     ]
    }
   ],
   "source": [
    "# tworzymy liste wszystkich slow w korpusie\n",
    "\n",
    "def get_all_words_sorted(corpus):\n",
    "    # generujemy posortowana alfabetycznie liste wszystkich slow (tokenow)\n",
    "    return sorted(\n",
    "        list(set([token for text in corpus for token in text]))\n",
    "    )\n",
    "#\n",
    "\n",
    "wordlist = get_all_words_sorted(stemmed_texts)\n",
    "word_indices = {x[1]: x[0] for x in enumerate(wordlist)}\n",
    "print(len(wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczamy liczbe tekstow, w ktorych wystapilo kazde ze slow\n",
    "# pamietaj, ze jesli slowo wystapilo w danym tekscie wielokrotnie, to liczymy je tylko raz\n",
    "\n",
    "def get_document_frequencies(corpus, wordlist):\n",
    "    # return {word -> count}\n",
    "    frequencies = defaultdict(lambda: 0)\n",
    "\n",
    "    # for word in wordlist:\n",
    "    #     for text in corpus:\n",
    "    #         if word in text:\n",
    "    #             frequencies[word] += 1\n",
    "    #             break \n",
    "\n",
    "    for text in corpus:\n",
    "        seen = set()\n",
    "        for token in text:\n",
    "            if token in seen: continue \n",
    "            seen.add(token)\n",
    "            frequencies[token] += 1\n",
    "    #\n",
    "            \n",
    "    return frequencies\n",
    "#\n",
    "\n",
    "document_frequency = get_document_frequencies(stemmed_texts, wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczamy liczbe wystapien kazdego slowa w kazdym tekscie\n",
    "\n",
    "def get_term_frequencies(corpus, news_indices):\n",
    "    # return {news_id -> {word -> count}}\n",
    "    frequencies = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    for news_id, text in zip(news_indices, corpus):\n",
    "        for token in text:\n",
    "            frequencies[news_id][token] += 1\n",
    "    #\n",
    "    \n",
    "    return frequencies\n",
    "#\n",
    "\n",
    "term_frequency = get_term_frequencies(stemmed_texts, news_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.get_term_frequencies.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "            {'think': 1,\n",
       "             'realli': 2,\n",
       "             'good': 2,\n",
       "             'team': 2,\n",
       "             'special': 1,\n",
       "             'thing': 1,\n",
       "             'group': 1,\n",
       "             'close': 1,\n",
       "             'brian': 1,\n",
       "             'schmetzer': 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sprawdzmy wyniki\n",
    "\n",
    "term_frequency[news_ids[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_tf_idf\u001b[39m(term_frequency, document_frequency, corpus_size):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# return {news_id -> {word -> tf_idf}}\u001b[39;00m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {news_id: {word: tf * math.log(corpus_size / df) \u001b[38;5;28;01mfor\u001b[39;00m word, tf \u001b[38;5;129;01min\u001b[39;00m tf_dict.items()} \u001b[38;5;28;01mfor\u001b[39;00m news_id, tf_dict \u001b[38;5;129;01min\u001b[39;00m term_frequency.items() \u001b[38;5;28;01mfor\u001b[39;00m word, df \u001b[38;5;129;01min\u001b[39;00m document_frequency.items() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tf_dict}\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tf_idf = \u001b[43mcalculate_tf_idf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm_frequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument_frequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnews_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mcalculate_tf_idf\u001b[39m\u001b[34m(term_frequency, document_frequency, corpus_size)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_tf_idf\u001b[39m(term_frequency, document_frequency, corpus_size):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# return {news_id -> {word -> tf_idf}}\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {news_id: {word: tf * math.log(corpus_size / df) \u001b[38;5;28;01mfor\u001b[39;00m word, tf \u001b[38;5;129;01min\u001b[39;00m tf_dict.items()} \u001b[38;5;28;01mfor\u001b[39;00m news_id, tf_dict \u001b[38;5;129;01min\u001b[39;00m term_frequency.items() \u001b[38;5;28;01mfor\u001b[39;00m word, df \u001b[38;5;129;01min\u001b[39;00m document_frequency.items() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tf_dict}\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def calculate_tf_idf(term_frequency, document_frequency, corpus_size):\n",
    "    # return {news_id -> {word -> tf_idf}}\n",
    "    return {news_id: {word: tf * math.log(corpus_size / df) for word, tf in tf_dict.items()} for news_id, tf_dict in term_frequency.items() for word, df in document_frequency.items() if word in tf_dict}\n",
    "\n",
    "tf_idf = calculate_tf_idf(term_frequency, document_frequency, len(news_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_idf\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m tf_idf = \u001b[43mcalculate_tf_idf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm_frequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument_frequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnews_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mcalculate_tf_idf\u001b[39m\u001b[34m(term_frequency, document_frequency, corpus_size)\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m token, count \u001b[38;5;129;01min\u001b[39;00m document_frequency.items():\n\u001b[32m      9\u001b[39m         tf = tf_dict[token] / \u001b[38;5;28msum\u001b[39m(tf_dict.values())\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m         idf = \u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m         tf_idf[news_id][token] = tf * idf\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# obliczamy metryke tf_idf\n",
    "\n",
    "def calculate_tf_idf(term_frequency, document_frequency, corpus_size):\n",
    "    # return {news_id -> {word -> tf_idf}}\n",
    "    tf_idf = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    for news_id, tf_dict in term_frequency.items():\n",
    "        for token, count in document_frequency.items():\n",
    "            tf = tf_dict[token] / sum(tf_dict.values())\n",
    "            idf = math.log(corpus_size / count)\n",
    "            tf_idf[news_id][token] = tf * idf\n",
    "    #\n",
    "\n",
    "    return tf_idf\n",
    "#\n",
    "\n",
    "tf_idf = calculate_tf_idf(term_frequency, document_frequency, len(news_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzmy wyniki\n",
    "\n",
    "tf_idf[news_ids[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 3. - Podobieństwo tekstów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczmy odleglosc miedzy dwoma artykulami\n",
    "# przetestuj rozne metryki odleglosci i wybierz najlepsza\n",
    "\n",
    "def calculate_distance(tf_idf, id1, id2):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "calculate_distance(tf_idf, news_ids[2], news_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja pomocnicza do wyswietlania artykulow\n",
    "def print_news_entry(n_id, corpus):\n",
    "    print(f'id: {n_id}\\n\\ttitle: {corpus[n_id][\"title\"]}\\n\\ttext: {corpus[n_id][\"abstract\"]}')\n",
    "\n",
    "print_news_entry('N42782', news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wyznaczmy k najpodobniejszych tekstow do danego\n",
    "# pamietaj o odpowiedniej kolejnosci sortowania w zaleznosci od wykorzystanej metryki\n",
    "# pamietaj, zeby wsrod podobnych tekstow nie bylo danego\n",
    "\n",
    "def get_k_most_similar_news(tf_idf, n_id, k):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def print_k_most_similar_news(tf_idf, n_id, k, corpus):\n",
    "    similar = get_k_most_similar_news(tf_idf, n_id, k)\n",
    "    print_news_entry(n_id, corpus)\n",
    "    print(f'\\n{k} most similar:')\n",
    "    for s_id in similar:\n",
    "       print_news_entry(s_id, corpus)\n",
    "\n",
    "print_k_most_similar_news(tf_idf, news_ids[42337], 5, news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 4. - Profile użytkowników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oblicz srednia z wektorow tf-idf artykulow o zadanych id-kach\n",
    "def calculate_average_vector(tf_idf, news_ids: list[str]) -> dict[str, float]:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# wykorzystaj powyzsza funkcje, by policzyc wektor kazdego uzytkownika\n",
    "def calculate_users_vectors(tf_idf, users_history) -> dict[str, list]:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "user_vectors = calculate_users_vectors(tf_idf, users_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdz wyliczony profil dla przykladowego uzytkownika\n",
    "print(sorted([(k,v) for k,v in user_vectors[test_users[0]].items() if v], key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skorzystajmy ze znanej juz biblioteki, by to lepiej zwizualizowac\n",
    "def plot_vector(tf_idf_vector):\n",
    "    wordcloud = WordCloud(random_state=42, background_color='black', colormap='Set2')\n",
    "    wordcloud.generate_from_frequencies(frequencies=tf_idf_vector)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "plot_vector(user_vectors[test_users[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 5. - Rekomendacje dla użytkowników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wykorzystujac wektory tresci i profile uzytkownikow,\n",
    "#   wygeneruj liste k artykulow najlepiej dopasowanych do uzytkownika\n",
    "#   pamietaj o odsianiu artykulow, ktore uzytkownik juz kliknal\n",
    "\n",
    "def recommend(tf_idf, user_id, news, users_history, k):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# dla wybranego uzytkownika, korzystajac z juz zaimplementowanych funkcji,\n",
    "#   pokaz jego historie, profil (wordcloud) i rekomendacje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Część 6. - Ocena jakości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jaccard index to metryka podobienstwa zbiorow, lekko ja zmodyfikujemy\n",
    "# przeciecie wektorow to minimum po kazdej wspolrzednej\n",
    "# unia wektorow to maksimum po kazdej wspolrzednej\n",
    "# jaccard index to iloraz sum tych dwoch wartosci\n",
    "\n",
    "def jaccard(v1, v2):\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dla kazdego uzytkownika wygeneruj k-elementowa rekomendacje\n",
    "# policz jaccard index miedzy wektorem uzytkownika a srednim wektorem elementow z rekomendacji\n",
    "# porownaj wyniki dla dwoch roznych k i dwoch roznych metryk podobienstwa\n",
    "\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
